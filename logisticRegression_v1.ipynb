{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lololololologistic regression!\n",
    "\n",
    "Dataset found here: [Red wine dataset](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)\n",
    "\n",
    "**Please run all of the data pipeline cells but then scoot straight on down to the exercises below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import scipy.optimize as opt  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70          0.0             1.9      0.076   \n",
       "1            7.8              0.88          0.0             2.6      0.098   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1599 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {df.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create two class labels based on the quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>class</th>\n",
       "      <th>class_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>paintStripper</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>paintStripper</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>paintStripper</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>midShelf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>paintStripper</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality          class  class_binary  \n",
       "0      9.4        5  paintStripper             0  \n",
       "1      9.8        5  paintStripper             0  \n",
       "2      9.8        5  paintStripper             0  \n",
       "3      9.8        6       midShelf             1  \n",
       "4      9.4        5  paintStripper             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create class labels (0=paintStripper, 1=midShelf)\n",
    "df['class'] = df['quality'].apply(lambda x: 'paintStripper' if x<=5 else 'midShelf')\n",
    "df['class_binary'] = df['quality'].apply(lambda x: 0 if x<=5 else 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "midShelf         855\n",
       "paintStripper    744\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty good distribution of classes so let's crack on\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the features\n",
    "\n",
    "sns.set_context(\"paper\", rc={\"axes.labelsize\":16})\n",
    "g = sns.pairplot(df.drop(['quality', 'class_binary'], axis=1), kind='scatter', hue='class', markers=[\"o\", \"s\"], corner=True,\n",
    "            plot_kws={'alpha':0.4})\n",
    "handles = g._legend_data.values()\n",
    "g.fig.legend(fontsize = 26, title = 'Class', title_fontsize = 26, handles=handles, labels=['Paint stripper', 'Mid-shelf'], loc='upper center', ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see a few things:\n",
    "1. There is a lot of overlap between the two classes, so let's not set our expectations too high;\n",
    "2. Some features do not seem to be of much help (e.g. pH), so let's drop a few;\n",
    "3. The decision boundaries do not appear too complex, so in terms of polynomials, we'll go with a second order;\n",
    "4. The features are at least normalish in their distributions, so is we wanted to scale, standard scaling would probably be fine (we're not going to do this, however, as all features are within one or two orders of magnitude anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop some features and split the data into our X and y\n",
    "\n",
    "# split data into features and class labels\n",
    "y = np.array(df['class_binary'])\n",
    "X = np.array(df.drop(['residual sugar', 'chlorides', 'free sulfur dioxide', 'pH', 'quality', 'class', 'class_binary'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A polynomial order of 2 has increased our number of features to 36, so be careful of overfitting.\n"
     ]
    }
   ],
   "source": [
    "# add higher order polynomial features\n",
    "\n",
    "poly_order = 2\n",
    "poly = PolynomialFeatures(degree=poly_order)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(f'A polynomial order of {poly_order} has increased our number of features to {X_poly.shape[1]}, so be careful of overfitting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a general rule, for any algorithm that uses gradient descent, you should consider scaling the features. We're not going to bother here though\n",
    "\n",
    "scale_features = 0\n",
    "\n",
    "if scale_features == 1:\n",
    "    # scale features: z = (x - u)/s, where x is a sample (data point), u and s are the mean and std of that feature, respectively \n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X_poly)\n",
    "else:\n",
    "    X_std = X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples (m): 1119;  test examples: 480\n"
     ]
    }
   ],
   "source": [
    "# add a column of ones for the bias term\n",
    "X_std = np.insert(X_std, 0, values=1, axis=1)\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3, random_state=66)\n",
    "print(f'Training examples (m): {X_train.shape[0]};  test examples: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for testing. These are placed here to tidy up the teaching exercises below\n",
    "\n",
    "def testCostFunction(lambda_reg):\n",
    "    # Initialize same parameters for testing\n",
    "    _, j = X_train.shape\n",
    "    theta_test = np.zeros((j,))\n",
    "\n",
    "    # load theta and cost for lambda 0\n",
    "    theta_filename = 'theta_lambda' + str(lambda_reg) + '.txt'\n",
    "    cost_filename = 'cost_lambda' + str(lambda_reg) + '.txt'\n",
    "    parameters_mat = np.loadtxt(theta_filename, delimiter=',')\n",
    "    cost_ary = np.loadtxt(cost_filename, delimiter=',')\n",
    "\n",
    "    print(f'Compare outputs from the cost function at lambda={lambda_reg}:', '\\n')\n",
    "\n",
    "    # fit loaded params to training data, calculate cost, and print\n",
    "    for i in range(parameters_mat.shape[0]):\n",
    "\n",
    "        theta_test = parameters_mat[i,:]\n",
    "        cost_Geo   = cost_ary[i]\n",
    "        cost_our   = costFunctionReg(theta_test, X_train, y_train, lambda_reg)\n",
    "\n",
    "        print(f'Geo: {round(cost_Geo, 5)}; Yours: {round(cost_our, 5)}')\n",
    "        \n",
    "def testGradFunction(lambda_reg):\n",
    "    # Initialize same parameters for testing\n",
    "    _, j = X_train.shape\n",
    "    theta_test = np.zeros((j,))\n",
    "\n",
    "    # load theta and cost for lambda 0\n",
    "    theta_filename = 'theta_lambda' + str(lambda_reg) + '.txt'\n",
    "    grad_filename = 'grad_lambda' + str(lambda_reg) + '.txt'\n",
    "    parameters_mat = np.loadtxt(theta_filename, delimiter=',')\n",
    "    grad_mat = np.loadtxt(grad_filename, delimiter=',')\n",
    "\n",
    "    print(f'Compare outputs from the partial derivative (grad) function at lambda={lambda_reg}:', '\\n')\n",
    "\n",
    "    # fit loaded params to training data, calculate sum of gradients, and print\n",
    "    for i in range(parameters_mat.shape[0]):\n",
    "        \n",
    "        theta_test = parameters_mat[i,:]\n",
    "        grad_Geo   = grad_mat[i, :].sum()\n",
    "        grad_our   = gradientReg(theta_test, X_train, y_train, lambda_reg)\n",
    "        grad_our   = grad_our.sum()\n",
    "\n",
    "        print(f'Geo: {round(grad_Geo, 5)}; Yours: {round(grad_our, 5)}')\n",
    "        \n",
    "# functions for plotting\n",
    "def plotSigmoid():\n",
    "    x_axis = np.linspace(-15, 15, 500)\n",
    "    y_sigmoid = sigmoid(x_axis)\n",
    "\n",
    "    plt.rc('xtick', labelsize=14) \n",
    "    plt.rc('ytick', labelsize=14) \n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(x_axis, y_sigmoid, 'tab:blue', linewidth=2)\n",
    "    plt.xlabel('theta^T . x')\n",
    "    plt.ylabel('h(x) = sigmoid(theta^T . x)')\n",
    "    plt.title('The sigmoid (logistic) function', fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# plot of cost function\n",
    "def plotCostFunc():\n",
    "    x_axis = np.linspace(0,1,101)\n",
    "    y_when_1 = -1*(np.log(x_axis))\n",
    "    y_when_0 = -1*(np.log(1 - x_axis))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
    "    fig.suptitle('Logistic regression cost function', fontsize=16)\n",
    "    ax1.plot(x_axis, y_when_0, 'tab:blue', linewidth=2)\n",
    "    ax2.plot(x_axis, y_when_1, 'tab:blue', linewidth=2)\n",
    "    ax1.set_title('if y=0, cost = -log(1 - hyp(x))', fontsize=16)\n",
    "    ax2.set_title('if y=1, cost = -log(hyp(x))', fontsize=16)\n",
    "    ax1.set_xlabel('hyp(x)')\n",
    "    ax2.set_xlabel('hyp(x)')\n",
    "    ax1.set_ylabel('cost')\n",
    "    ax2.set_ylabel('cost')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise starts here!\n",
    "\n",
    "At this stage, the data has been tidied up a bit and split into our train and test subsets X_train, y_train, X_test, y_test â€” all are numpy arrays. Your task is to:\n",
    "\n",
    "1. Complete the costFunctionReg function\n",
    "2. Complete the gradientReg function\n",
    "3. Complete the predict function\n",
    "4. Enjoy yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.53978687e-05 2.68941421e-01 5.00000000e-01 7.31058579e-01\n",
      " 9.99954602e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This has been provided for simplicity, but you should test some outputs. You should find that it works with arrays as well as numbers, and that large negative numbers -> 0,\n",
    "# and large positive numbers -> 1\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    g = 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    return g\n",
    "\n",
    "print(sigmoid(np.array([-10, -1, 0, 1, 10])))\n",
    "plotSigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Complete the cost function\n",
    "\n",
    "The very first step in the logistic regression algorithm is to initialize some values for your parameters (commonly this will just be zeroes). But the next step is to calculate the cost of this model, at those parameters, when fitted to the training data. And this is what you are now going to do below.\n",
    "\n",
    "Remember, the cost function is of the form: <br>\n",
    "\n",
    "**J(theta) = -1/m * (SUM_i:m(y_i*log(hyp(x_i)) + (1-y_i)log(1-hyp(x_i))) + lambda/2m * SUM_j:n(theta_j^2)**, <br> where i=1:m are all training examples, j=1:n are all features (excluding the bias, i.e. the first feature, j_0), and hyp(x_i) is our prediction for training example i (i.e. sigmoid(theta^T.x))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotCostFunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, lambda_reg):\n",
    "\n",
    "    # num training examples\n",
    "    m = len(y)\n",
    "\n",
    "    # You need to calculate the cost and regularization term\n",
    "    J = 0\n",
    "    reg_term = 0\n",
    "    \n",
    "    ##### PART 1: Calculate the cost, J without the reg term #####\n",
    "    \n",
    "    # calculate the unregulated cost term\n",
    "    hypothesis_vec = sigmoid(np.dot(X, theta))\n",
    "    J_unreg = sum((-1*y)*np.log(hypothesis_vec) - (1-y)*np.log(1 - hypothesis_vec))/m\n",
    "    \n",
    "    \n",
    "    # CHECK YOUR CODE BY RUNNING AGAINST THE TEST SET WITH LAMBDA=0\n",
    "    \n",
    "    \n",
    "    ##### PART 2: Calculate the reg term and add it to J #####\n",
    "    \n",
    "    # add in L2 regularization ('Ridge Regularization')\n",
    "    theta[0] = 0\n",
    "    reg_term = np.dot(theta.T, theta) * (lambda_reg/(2*m))\n",
    "    \n",
    "    # total cost\n",
    "    J = J_unreg + reg_term\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your cost function\n",
    "\n",
    "When you think you have working code for part 1, you can test your results against my own. To test part 1, run this function with lambda=0. When you're ready to test your code with the regularization term, I've provided test cases for lambda = 0, 1, 10, 100, and 1000 (you'll get an error if you try anything else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare outputs from the cost function at lambda=10: \n",
      "\n",
      "Geo: 0.62175; Yours: 0.62175\n",
      "Geo: 0.58891; Yours: 0.58891\n",
      "Geo: 0.5282; Yours: 0.5282\n",
      "Geo: 0.51029; Yours: 0.51029\n",
      "Geo: 0.49858; Yours: 0.49858\n",
      "Geo: 0.49845; Yours: 0.49845\n"
     ]
    }
   ],
   "source": [
    "# choose lambda = 0, 1, 10, 100, or 1000 (anything else will give an error)\n",
    "\n",
    "### ADJUST THIS PARAM ###\n",
    "lambda_reg = 10\n",
    "#########################\n",
    "        \n",
    "testCostFunction(lambda_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete the partial derivatives function (gradientReg)\n",
    "\n",
    "Now that you've got a working cost function, you need to calculate the partial derivatives of the cost function with respect to each parameter. Again, this can be done in two parts, first, without the regularization component, and then with. You can test your function against my own below (same as for the cost).\n",
    "\n",
    "Your function should return an array, grad, with lenth=number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientReg(theta, X, y, lambda_reg):\n",
    "    \n",
    "    ##### PART 1: Calculate the partial derivs for each parameter #####\n",
    "    \n",
    "    # num training examples\n",
    "    m = len(y)\n",
    "    \n",
    "    # calculate unregularized term of the partial derivs\n",
    "    hypothesis_vec = sigmoid(np.dot(X, theta))\n",
    "    grad = np.dot((hypothesis_vec - y).T, X)\n",
    "    grad = grad * (1/m)\n",
    "    \n",
    "    # CHECK YOUR CODE BY RUNNING AGAINST THE TEST SET WITH LAMBDA=0\n",
    "    \n",
    "    ##### PART 2: include the reg term #####\n",
    "    \n",
    "    # add in the regularized terms\n",
    "    theta[0] = 0 # do not regularize first term\n",
    "    grad = grad + (theta * (lambda_reg/m))\n",
    "\n",
    "    return grad # should be a vector with length equal to the number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your partial derivative function (gradientReg)\n",
    "\n",
    "This is the same as before. You can test for your gradient function without regularization (i.e. at lambda=0), and at lambda=1, 10, 100, aqnd 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare outputs from the partial derivative (grad) function at lambda=1000: \n",
      "\n",
      "Geo: 114.30384; Yours: 114.30384\n",
      "Geo: -59.96628; Yours: -59.96628\n",
      "Geo: -72.87377; Yours: -72.87377\n",
      "Geo: 1.74338; Yours: 1.74338\n",
      "Geo: -8.84228; Yours: -8.84228\n",
      "Geo: -8.84228; Yours: -8.84228\n"
     ]
    }
   ],
   "source": [
    "# choose lambda = 0, 1, 10, 100, or 1000 (anything else will give an error)\n",
    "\n",
    "### ADJUST THIS PARAM ###\n",
    "lambda_reg = 1000\n",
    "#########################\n",
    "        \n",
    "testGradFunction(lambda_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Now we have the components required for gradient descent, we just have to do it. However, as the code for implementing optimizers is not really very interesting, I've just done this for you (see below). But anyway, after running the optimizer, we don't get our predictions, just our optimum parameters for the generalised linear model (i.e., we've found values for theta that minimize the cost, and we've done this using the partial derivatives). Or in Scikit-learn language, we've just done our .fit(). Now, we need to do our .predict(), and for this, you need to complete the final function (see further below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is actually doing the gradient descent\n",
    "\n",
    "def optimizeTheta(initial_theta, X, y, lambda_reg):\n",
    "    \n",
    "    solution = opt.fmin_tnc(func=costFunctionReg, x0=initial_theta, fprime=gradientReg, args=(X, y, lambda_reg))\n",
    "    theta_fit = solution[0]\n",
    "    iterations = solution[1]\n",
    "    \n",
    "    return theta_fit, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict the class labels (1 or 0)\n",
    "\n",
    "Complete the function below. If the hypothesis for a given training example is 0.5 or more, assign it 1, otherwise, assign it 0.\n",
    "\n",
    "I.e.: <br>\n",
    "if hyp(xi)>=0.5, y_pred=1 <br>\n",
    "if hyp(xi)< 0.5, y_pred=0\n",
    "\n",
    "You should return a list of predictions with length = number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    \n",
    "    probability = sigmoid(np.dot(X, theta))\n",
    "    predictions = [1 if x >= 0.5 else 0 for x in probability]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please just run these cells and skip to the next section. You can take a look at them later, but let's run our models first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provided a classification accuracy function for simplicity\n",
    "def classificationAccuracy(y_actual, y_hypothesis):\n",
    "    correct = []\n",
    "    for y_act, y_hyp in zip(y_actual, y_hypothesis):\n",
    "        if int(y_act)==int(y_hyp):\n",
    "            correct.append(1)\n",
    "        else:\n",
    "            correct.append(0)\n",
    "    \n",
    "    accuracy = sum(correct)/len(correct)\n",
    "    return accuracy\n",
    "\n",
    "# also provided this ugly function. It just uses all of the functions that you've made to fit a logistic regression classifier, and it compares the results to SK-learn\n",
    "def fitAllModels(X_train, y_train, X_test, y_test, lambda_reg):\n",
    "    \n",
    "    ### RUN OUR IMPLEMENTATION OF THE LOGISTIC REGRESSION ALGORITHM ###\n",
    "    \n",
    "    # Initialize fitting parameters\n",
    "    _, j = X_train.shape\n",
    "    theta = np.zeros((j,))\n",
    "    \n",
    "    # minimize the cost function (this is our .fit() using the Scikit-learn package)\n",
    "    theta_fit, iterations = optimizeTheta(theta, X_train, y_train, lambda_reg)\n",
    "\n",
    "    # get accuracy scores for the test set\n",
    "    y_test_pred = predict(theta_fit, X_test)\n",
    "    accuracy_test = classificationAccuracy(y_test, y_test_pred)\n",
    "\n",
    "    #Â same again for the train\n",
    "    y_train_pred = predict(theta_fit, X_train)\n",
    "    accuracy_train = classificationAccuracy(y_train, y_train_pred)\n",
    "\n",
    "    # calculate the cost at the optimum theta values\n",
    "    J_train = costFunctionReg(theta_fit, X_train, y_train, lambda_reg)\n",
    "\n",
    "    print(f'Our logistic regression accuracy for train: {round(accuracy_train*100)}%, and for test: {round(accuracy_test*100)}%. Solution found in {iterations} iterations.')\n",
    "\n",
    "    ### CHECK AGAINST SK-LEARN ###\n",
    "    \n",
    "    # you can think of C as inversely proportional to lambda (it's a formalism usually used with SVMs, but with SK-learn they also use it with the log reg package)\n",
    "    C_param = 1/lambda_reg if lambda_reg>0 else 1_000_000 #<- this ensures we don't accidentally divide by zero\n",
    "\n",
    "    # C is our regularization param; and we do not need to add the bias (as we have already done this manually by adding ones to X)\n",
    "    LR_model = LogisticRegression(C=C_param, solver='liblinear', penalty='l2', max_iter=1000, fit_intercept=False) \n",
    "    LR_model.fit(X_train, y_train)\n",
    "    theta_SK = LR_model.coef_\n",
    "    theta_SK = theta_SK.T.reshape(-1)\n",
    "    iterations_SK = LR_model.n_iter_[0]\n",
    "    y_pred_test_SK = LR_model.predict(X_test)\n",
    "    y_pred_train_SK = LR_model.predict(X_train)\n",
    "\n",
    "    accuracy_test_SK = classificationAccuracy(y_test, y_pred_test_SK)\n",
    "    accuracy_train_SK = classificationAccuracy(y_train, y_pred_train_SK)\n",
    "\n",
    "    # calculate the cost at the optimum theta val from SK-learn\n",
    "    J_train_SK = costFunctionReg(theta_SK, X_train, y_train, lambda_reg)\n",
    "\n",
    "    print(f'SK-learn accuracy for train:                {round(accuracy_train_SK*100)}%, and for test: {round(accuracy_test_SK*100)}%. Solution found in {iterations_SK} iterations.')\n",
    "    print('\\n')\n",
    "    print(f' Cost from our log reg: {round(J_train, 5)}', '\\n', f'Cost from SK-learn:    {round(J_train_SK, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model\n",
    "\n",
    "We have everything in place now to both train and evaluate our logistic regression model. Let's run it and compare the results against Sci-kit learn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our logistic regression accuracy for train: 68%, and for test: 65%. Solution found in 131 iterations.\n",
      "SK-learn accuracy for train:                68%, and for test: 65%. Solution found in 15 iterations.\n",
      "\n",
      "\n",
      " Cost from our log reg: 0.60056 \n",
      " Cost from SK-learn:    0.60038\n"
     ]
    }
   ],
   "source": [
    "### CHOOSE ANY LAMBDA AND THEN RUN ###\n",
    "lambda_reg = 50000\n",
    "\n",
    "fitAllModels(X_train, y_train, X_test, y_test, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # implementing logistic regression with for-loops\n",
    "\n",
    "# def costFunctionReg_loop(theta, X, y, lambda_reg):\n",
    "    \n",
    "#     # num training examples\n",
    "#     m = len(y)\n",
    "    \n",
    "#     # You need to return the following variables correctly\n",
    "#     J = 0\n",
    "    \n",
    "#     # calculate the unregulated cost term\n",
    "#     J_unreg = 0\n",
    "#     for i in range(m):\n",
    "        \n",
    "#         # x_i, y_i, and y_pred (hypothesis)\n",
    "#         x_i = X[i,:]\n",
    "#         y_i = y[i]\n",
    "#         hypothesis_i = sigmoid(np.dot(x_i, theta))\n",
    "        \n",
    "#         J_unreg_i = ((-1*y_i)*np.log(hypothesis_i)) - ((1 - y_i)*(np.log(1 - hypothesis_i)))\n",
    "#         J_unreg += J_unreg_i\n",
    "        \n",
    "#     J_unreg = J_unreg/m # when out of loop, take average\n",
    "    \n",
    "#     # calculate the regularization term\n",
    "#     reg_term = 0\n",
    "#     for i, theta_j in enumerate(theta):\n",
    "#         # skip the bias term\n",
    "#         if i==0:\n",
    "#             continue \n",
    "#         reg_term = reg_term + theta_j**2\n",
    "        \n",
    "#     # moderate the reg term with lambda (the regularization parameter)\n",
    "#     reg_term = (lambda_reg/(2*m)) * reg_term\n",
    "    \n",
    "#     # total cost\n",
    "#     J = J_unreg + reg_term\n",
    "    \n",
    "#     return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize some parameters for testing\n",
    "# _, j = X_train.shape\n",
    "# theta_test = np.ones((j,))*0.000000000001\n",
    "# lambda_reg = 0.0000000000001\n",
    "\n",
    "# print(costFunctionReg(theta_test, X_train, y_train, lambda_reg))\n",
    "# print(costFunctionReg_loop(theta_test, X_train, y_train, lambda_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize some parameters for testing\n",
    "# _, j = X_train.shape\n",
    "# theta_test = np.ones((j,))*0.000000000001\n",
    "# lambda_reg = 100000000000\n",
    "\n",
    "# print(costFunctionReg(theta_test, X_train, y_train, lambda_reg))\n",
    "# print(costFunctionReg_loop(theta_test, X_train, y_train, lambda_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check that your cost function works (without regularization, i.e. lambda=0)\n",
    "\n",
    "# # Initialize some parameters for testing\n",
    "# _, j = X_train.shape\n",
    "# theta_test = np.zeros((j,))\n",
    "# lambda_reg = 1000\n",
    "\n",
    "# # load theta and cost for lambda 0\n",
    "# theta_filename = 'theta_lambda' + str(lambda_reg) + '.txt'\n",
    "# cost_filename = 'cost_lambda' + str(lambda_reg) + '.txt'\n",
    "# parameters_mat = np.loadtxt(theta_filename, delimiter=',')\n",
    "# cost_ary = np.loadtxt(cost_filename, delimiter=',')\n",
    "\n",
    "# print(f'Compare results at lambda={lambda_reg}:', '\\n')\n",
    "\n",
    "# for i in range(parameters_mat.shape[0]):\n",
    "    \n",
    "#     theta_test = parameters_mat[i,:]\n",
    "#     cost_Geo = cost_ary[i]\n",
    "    \n",
    "#     cost_our = costFunctionReg(theta_test, X_train, y_train, lambda_reg)\n",
    "    \n",
    "    \n",
    "#     print(f'Geo: {round(cost_Geo, 5)}; Yours: {round(cost_our, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradientReg(theta, X, y, lambda_reg):\n",
    "    \n",
    "#     ##### PART 2: Calculate the partial derivs for each parameter #####\n",
    "    \n",
    "#     # num training examples\n",
    "#     m = len(y)\n",
    "    \n",
    "#     # calculate unregularized term of the partial derivs\n",
    "#     hypothesis_vec = sigmoid(np.dot(X, theta))\n",
    "#     grad = np.dot((hypothesis_vec - y).T, X)\n",
    "#     grad = grad * (1/m)\n",
    "    \n",
    "#     # add in the regularized terms\n",
    "#     theta[0] = 0 # do not regularize first term\n",
    "#     grad = grad + (theta * (lambda_reg/m))\n",
    "\n",
    "#     return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradientReg(theta, X, y, lambda_reg):\n",
    "    \n",
    "#     ##### PART 2: Calculate the partial derivs for each parameter #####\n",
    "    \n",
    "#     # num training examples\n",
    "#     m = len(y)\n",
    "    \n",
    "#     # calculate unregularized term of the partial derivs\n",
    "#     hypothesis_vec = sigmoid(np.matmul(X, theta))\n",
    "#     grad = np.matmul(np.transpose(hypothesis_vec - y), X)\n",
    "#     grad = grad * (1/m)\n",
    "    \n",
    "#     # add in the regularized terms\n",
    "#     theta[0] = 0 # do not regularize first term\n",
    "#     grad = grad + (theta * (lambda_reg/m))\n",
    "\n",
    "#     return grad\n",
    "\n",
    "# def optimizeTheta(initial_theta, X, y, lambda_reg):\n",
    "    \n",
    "#     solution = opt.fmin_tnc(func=costFunctionReg, x0=initial_theta, fprime=gradientReg, args=(X, y, lambda_reg))\n",
    "#     theta_fit = solution[0]\n",
    "#     iterations = solution[1]\n",
    "    \n",
    "#     return theta_fit, iterations\n",
    "\n",
    "# def optimizeTheta_maxIter(initial_theta, X, y, lambda_reg, max_iter):\n",
    "    \n",
    "#     solution = opt.fmin_tnc(func=costFunctionReg, x0=initial_theta, fprime=gradientReg, args=(X, y, lambda_reg), maxfun=max_iter)\n",
    "#     theta_fit = solution[0]\n",
    "#     iterations = solution[1]\n",
    "    \n",
    "#     return theta_fit, iterations\n",
    "\n",
    "# def predict(theta, X):\n",
    "    \n",
    "#     probability = sigmoid(np.dot(X, theta))\n",
    "#     predictions = [1 if x >= 0.5 else 0 for x in probability]\n",
    "#     return predictions\n",
    "\n",
    "# def classificationAccuracy(y_actual, y_hypothesis):\n",
    "#     correct = []\n",
    "#     for y_act, y_hyp in zip(y_actual, y_hypothesis):\n",
    "#         if int(y_act)==int(y_hyp):\n",
    "#             correct.append(1)\n",
    "#         else:\n",
    "#             correct.append(0)\n",
    "    \n",
    "#     accuracy = sum(correct)/len(correct)\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check that \n",
    "\n",
    "# max_iters = [5, 10, 20, 40, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize fitting parameters\n",
    "# _, j = X_train.shape\n",
    "# initial_theta = np.zeros((j,))\n",
    "# # set lambda_reg\n",
    "# lambda_reg = 0\n",
    "\n",
    "# fitAllModels(initial_theta, X_train, y_train, X_test, y_test, lambda_reg)\n",
    "\n",
    "# def fitAllModels(theta, X_train, y_train, X_test, y_test, lambda_reg):\n",
    "    \n",
    "#     # Initialize fitting parameters\n",
    "#     _, j = X_train.shape\n",
    "#     initial_theta = np.zeros((j,))\n",
    "    \n",
    "#     # minimize the cost function (this is our .fit() using the Scikit-learn package)\n",
    "#     theta_fit, iterations = optimizeTheta(theta, X_train, y_train, lambda_reg)\n",
    "\n",
    "#     # get accuracy scores for train and test\n",
    "#     y_test_pred = predict(theta_fit, X_test)\n",
    "#     accuracy_test = classificationAccuracy(y_test, y_test_pred)\n",
    "\n",
    "#     y_train_pred = predict(theta_fit, X_train)\n",
    "#     accuracy_train = classificationAccuracy(y_train, y_train_pred)\n",
    "\n",
    "#     # calculate the cost at the optimum theta val\n",
    "#     J_train = costFunctionReg(theta_fit, X_train, y_train, lambda_reg)\n",
    "\n",
    "#     print(f'Our logistic regression accuracy for train: {round(accuracy_train*100)}%, and for test: {round(accuracy_test*100)}%. Solution found in {iterations} iterations.')\n",
    "\n",
    "#     # check against SK-learn\n",
    "\n",
    "#     # you can think of C as inversely proportional to lambda (it's a formalism usually used with SVMs, but with SK-learn they also use it with the log reg package)\n",
    "#     C_param = 1/lambda_reg if lambda_reg>0 else 1_000_000 #<- this ensures we don't accidentally divide by zero\n",
    "\n",
    "#     LR_model = LogisticRegression(C=C_param, solver='liblinear', penalty='l2', max_iter=1000, fit_intercept=False) # C is our regularization param; and we do not need to add the bias (as we have already done this manually by adding ones to X)\n",
    "#     LR_model.fit(X_train, y_train)\n",
    "#     theta_SK = LR_model.coef_\n",
    "#     theta_SK = theta_SK.T.reshape(-1)\n",
    "#     iterations_SK = LR_model.n_iter_[0]\n",
    "#     y_pred_test_SK = LR_model.predict(X_test)\n",
    "#     y_pred_train_SK = LR_model.predict(X_train)\n",
    "\n",
    "#     accuracy_test_SK = classificationAccuracy(y_test, y_pred_test_SK)\n",
    "#     accuracy_train_SK = classificationAccuracy(y_train, y_pred_train_SK)\n",
    "\n",
    "#     # calculate the cost at the optimum theta val from SK-learn\n",
    "#     J_train_SK = costFunctionReg(theta_SK, X_train, y_train, lambda_reg)\n",
    "\n",
    "#     print(f'SK-learn accuracy for train:                {round(accuracy_train_SK*100)}%, and for test: {round(accuracy_test_SK*100)}%. Solution found in {iterations_SK} iterations.')\n",
    "#     print('\\n')\n",
    "#     print(f' Cost from our log reg: {round(J_train, 5)}', '\\n', f'Cost from SK-learn:    {round(J_train_SK, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize fitting parameters\n",
    "# _, j = X_train.shape\n",
    "# initial_theta = np.zeros((j,))\n",
    "\n",
    "# # set lambda_reg\n",
    "# lambda_reg = 0\n",
    "\n",
    "# # minimize the cost function (this is our .fit() using the Scikit-learn package)\n",
    "# theta_fit, iterations = optimizeTheta(initial_theta, X_train, y_train, lambda_reg)\n",
    "\n",
    "# # get accuracy scores for train and test\n",
    "# y_test_pred = predict(theta_fit, X_test)\n",
    "# accuracy_test = classificationAccuracy(y_test, y_test_pred)\n",
    "\n",
    "# y_train_pred = predict(theta_fit, X_train)\n",
    "# accuracy_train = classificationAccuracy(y_train, y_train_pred)\n",
    "\n",
    "# # calculate the cost at the optimum theta val\n",
    "# J_train = costFunctionReg(theta_fit, X_train, y_train, lambda_reg)\n",
    "\n",
    "# print(f'Our logistic regression accuracy for train: {round(accuracy_train*100)}%, and for test: {round(accuracy_test*100)}%. Solution found in {iterations} iterations.')\n",
    "\n",
    "# # check against SK-learn\n",
    "\n",
    "# # you can think of C as inversely proportional to lambda (it's a formalism usually used with SVMs, but with SK-learn they also use it with the log reg package)\n",
    "# C_param = 1/lambda_reg if lambda_reg>0 else 1_000_000 #<- this ensures we don't accidentally divide by zero\n",
    "\n",
    "# LR_model = LogisticRegression(C=C_param, solver='liblinear', penalty='l2', max_iter=1000, fit_intercept=False) # C is our regularization param; and we do not need to add the bias (as we have already done this manually by adding ones to X)\n",
    "# LR_model.fit(X_train, y_train)\n",
    "# theta_SK = LR_model.coef_\n",
    "# theta_SK = theta_SK.T.reshape(-1)\n",
    "# iterations_SK = LR_model.n_iter_[0]\n",
    "# y_pred_test_SK = LR_model.predict(X_test)\n",
    "# y_pred_train_SK = LR_model.predict(X_train)\n",
    "\n",
    "# accuracy_test_SK = classificationAccuracy(y_test, y_pred_test_SK)\n",
    "# accuracy_train_SK = classificationAccuracy(y_train, y_pred_train_SK)\n",
    "\n",
    "# # calculate the cost at the optimum theta val from SK-learn\n",
    "# J_train_SK = costFunctionReg(theta_SK, X_train, y_train, lambda_reg)\n",
    "\n",
    "# print(f'SK-learn accuracy for train:                {round(accuracy_train_SK*100)}%, and for test: {round(accuracy_test_SK*100)}%. Solution found in {iterations_SK} iterations.')\n",
    "# print('\\n')\n",
    "# print(f' Cost from our log reg: {round(J_train, 5)}', '\\n', f'Cost from SK-learn:    {round(J_train_SK, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # re-do the train test split and assign predicted labels for plotting\n",
    "# X_train_plot, X_test_plot, y_train_plot, y_test_plot = train_test_split(df, y, test_size=0.3, random_state=66)\n",
    "\n",
    "# X_train_plot['predictions_binary'] = list(y_train_pred)\n",
    "# X_test_plot['predictions_binary'] = list(y_test_pred)\n",
    "\n",
    "# X_train_plot['predictions'] = X_train_plot['predictions_binary'].apply(lambda x: 'paintStripper' if x==0 else 'midShelf')\n",
    "# X_test_plot['predictions'] = X_test_plot['predictions_binary'].apply(lambda x: 'paintStripper' if x==0 else 'midShelf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_context(\"paper\", rc={\"axes.labelsize\":16})\n",
    "# g = sns.pairplot(X_test_plot.drop(['quality', 'class_binary', 'class', 'predictions_binary'], axis=1), kind='scatter', hue='predictions', markers=[\"o\", \"s\"], corner=True,\n",
    "#             plot_kws={'alpha':0.4})\n",
    "# handles = g._legend_data.values()\n",
    "# g.fig.legend(fontsize = 26, title = 'Predictions', title_fontsize = 26, handles=handles, labels=['Paint stripper', 'Mid-shelf'], loc='upper center', ncol=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the sigmoid function\n",
    "\n",
    "# x_axis = np.linspace(-15, 15, 500)\n",
    "# y_sigmoid = sigmoid(x_axis)\n",
    "\n",
    "# plt.rc('xtick', labelsize=14) \n",
    "# plt.rc('ytick', labelsize=14) \n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.plot(x_axis, y_sigmoid, 'tab:blue', linewidth=2)\n",
    "# plt.xlabel('theta^T . x')\n",
    "# plt.ylabel('h(x) = sigmoid(theta^T . x)')\n",
    "# plt.title('The sigmoid (logistic) function', fontsize=16)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot of cost function\n",
    "\n",
    "# x_axis = np.linspace(0,1,101)\n",
    "# y_when_1 = -1*(np.log(x_axis))\n",
    "# y_when_0 = -1*(np.log(1 - x_axis))\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
    "# fig.suptitle('Logistic regression cost function', fontsize=16)\n",
    "# ax1.plot(x_axis, y_when_0, 'tab:blue', linewidth=2)\n",
    "# ax2.plot(x_axis, y_when_1, 'tab:blue', linewidth=2)\n",
    "# ax1.set_title('if y=0, cost = -log(1 - hyp(x))', fontsize=16)\n",
    "# ax2.set_title('if y=1, cost = -log(hyp(x))', fontsize=16)\n",
    "# ax1.set_xlabel('hyp(x)')\n",
    "# ax2.set_xlabel('hyp(x)')\n",
    "# ax1.set_ylabel('cost')\n",
    "# ax2.set_ylabel('cost')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # implementing logistic regression with for-loops\n",
    "\n",
    "# def costFunctionReg_loop(theta, X, y, lambda_reg):\n",
    "    \n",
    "#     # num training examples\n",
    "#     m = len(y)\n",
    "    \n",
    "#     # You need to return the following variables correctly\n",
    "#     J = 0\n",
    "#     grad = np.zeros(theta.shape)\n",
    "    \n",
    "#     # calculate the unregulated cost term\n",
    "#     J_unreg = 0\n",
    "#     for i in range(m):\n",
    "        \n",
    "#         # x_i, y_i, and y_pred (hypothesis)\n",
    "#         x_i = X[i,:]\n",
    "#         y_i = y[i]\n",
    "#         hypothesis_i = sigmoid(np.matmul(x_i, theta))\n",
    "        \n",
    "#         J_unreg_i = ((-1*y_i)*np.log(hypothesis_i)) - ((1 - y_i)*(np.log(1 - hypothesis_i)))\n",
    "#         J_unreg += J_unreg_i\n",
    "        \n",
    "#     J_unreg = J_unreg/m # when out of loop, take average\n",
    "    \n",
    "#     # calculate the regularization term\n",
    "#     reg_term = 0\n",
    "#     for i, theta_j in enumerate(theta):\n",
    "#         # skip the bias term\n",
    "#         if i==0:\n",
    "#             continue \n",
    "#         reg_term = reg_term + theta_j**2\n",
    "        \n",
    "#     # moderate the reg term with lambda (the regularization parameter)\n",
    "#     reg_term = (lambda_reg/(2*m)) * reg_term\n",
    "    \n",
    "#     # total cost\n",
    "#     J = J_unreg + reg_term\n",
    "    \n",
    "#     return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize fitting parameters\n",
    "# _, j = X_train.shape\n",
    "# initial_theta = np.zeros((j,))\n",
    "# # set lambda_reg\n",
    "# lambda_reg = 1000\n",
    "\n",
    "# max_iters = [5, 10, 20, 40, 80]\n",
    "# Theta_mat = np.zeros((len(max_iters)+1, j))\n",
    "# grad_mat = np.zeros((len(max_iters)+1, j))\n",
    "# J_ary = np.zeros((len(max_iters)+1, ))\n",
    "\n",
    "# for i, max_iter in enumerate(max_iters):\n",
    "    \n",
    "#     # minimize the cost function up to max iter\n",
    "#     theta_fit, iterations = optimizeTheta_maxIter(initial_theta, X_train, y_train, lambda_reg, max_iter)\n",
    "#     Theta_mat[i, :] = theta_fit\n",
    "    \n",
    "#     # calculate the cost\n",
    "#     cost = costFunctionReg(theta_fit, X_train, y_train, lambda_reg)\n",
    "#     J_ary[i,] = cost\n",
    "    \n",
    "#     # calculate the gradient\n",
    "#     grad = gradientReg(theta_fit, X_train, y_train, lambda_reg)\n",
    "#     grad_mat[i, :] = grad\n",
    "\n",
    "# # minimize the cost function \n",
    "# theta_fit, iterations = optimizeTheta(initial_theta, X_train, y_train, lambda_reg)\n",
    "# Theta_mat[len(max_iters), :] = theta_fit\n",
    "# # print(Theta_mat)\n",
    "\n",
    "# # calculate the cost\n",
    "# cost = costFunctionReg(theta_fit, X_train, y_train, lambda_reg)\n",
    "# J_ary[len(max_iters)] = cost\n",
    "# # print(J_ary)\n",
    "\n",
    "# # calculate the grad\n",
    "# grad = gradientReg(theta_fit, X_train, y_train, lambda_reg)\n",
    "# grad_mat[len(max_iters), :] = grad\n",
    "    \n",
    "# np.savetxt('theta_lambda1000.txt', Theta_mat, delimiter=',')   \n",
    "# np.savetxt('cost_lambda1000.txt', J_ary, delimiter=',')   \n",
    "# np.savetxt('grad_lambda1000.txt', grad_mat, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
